{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sriteja Reddy Pashya (2021111019)\n",
    "\n",
    "- Romica Raisinghani (2021101053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem  (Assignment 3, TAO, Spring 2023)\n",
    "### Instructor: Dr. Pawan Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Problem\n",
    "### Given a set of input vectors corresponding to objects (or featues) decide which of the N classes the object belongs to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference (some figures for illustration below are taken from this): \n",
    "1. SVM without Tears, https://med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf\n",
    "2. SVM is one of the most popular methods in machine learning\n",
    "3. The dataset iris required is in the zip folder of assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Methods for Classification Problem\n",
    "1. Perceptron\n",
    "2. SVM\n",
    "3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will briefly describe the idea behind support vector machines for classification problems. We first describe linear SVM used to classify linearly separable data, and then we describe how we can use these algorithm for non-linearly separable data by so called kernels. The kernels are functions that map non-linearly separable data to a space usually higher dimensional space where the data becomes linearly separable. Let us quickly start with linear SVM.\n",
    "\n",
    "### Linear SVM for two class classification\n",
    "We recall the separating hyperpplane theorem: If there are two non-intersecting convex set, then there exists a hyperplane that separates the two convex sets. This is the assumption we will make: we assume that the convex hull of the given data leads to two convex sets for two classes, such that a hyperplane exists that separates the convex hulls. \n",
    "\n",
    "### Main idea of SVM: \n",
    "Not just find a hyperplane (as in perceptrons), but find one that keeps a good (largest possible) gap from the the data samples of each class. This gap is popularly called margins.\n",
    "\n",
    "### Illustration of problem, and kewords\n",
    "Consider the dataset of cancer and normal patients, hence it is a two class problem. Let us visualize the data:\n",
    "\n",
    "<img src=\"svmt1.png\" width=\"550\">\n",
    "\n",
    "Let us notice the following about the given data:\n",
    "0. There are two classes: blue shaded stars and red shaded circles.\n",
    "2. The input vector is two dimensional, hence it is of the form $(x_1^1, x_2^1).$\n",
    "2. Here $x_1^1, x_2^2$ are values of the features corresponding to two gene features: Gene X, Gene Y.\n",
    "3. Here red line is the linear classifier or hyperplane that separates the given input data.\n",
    "4. There are two dotted lines: one passes through a blue star point, and another dotted line passes through two red shaded circle points.\n",
    "5. The distance between the two dotted lines is called gap or margin that we mentioned before.\n",
    "6. Goal of SVM compared to perceptrons is to maximize this margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation of Optimization Model for Linear SVM\n",
    "We now assume that the red hyperplane above with maximum margin is given by $$w \\cdot x + b,$$\n",
    "We further assume that the dotted lines above are given by $$w \\cdot x + b = -1, \\quad w \\cdot x + b = +1.$$\n",
    "\n",
    "<img src=\"svmt2.png\" width=\"400\">\n",
    "\n",
    "For reasons, on why we can choose such hyperplane is shown in slides Lecture 16 of TAO. Since we want to maximize the margin the distance between the dotted lines, we recall the formula for diatance between planes. Let $D$ denote the distance, then \n",
    "$$D = 2/ \\| w \\|.$$\n",
    "So, to maximize the margin $D,$ we need to minimize $\\| w \\|.$ For convenience of writing algorithm (for differentiable function), we can say that minimizing $\\| w \\|$ is equivalent to minimizing $1/2 \\| w \\|^2.$ Hence \n",
    "### Objective function: $\\dfrac{1}{2} \\| w \\|^2$\n",
    "For our hyperplane to classify correctly, we need points of one class on one side of dotted line, more concretely\n",
    "$$w \\cdot x + b \\leq -1,$$\n",
    "and the we want the samples of another class (red ones) be on the other side of other dotted lines, i.e., \n",
    "$$ w \\cdot x + b \\geq +1.$$\n",
    "Let us now look what constraints mean in figure:\n",
    "\n",
    "<img src=\"svmt3.png\" width=\"400\">\n",
    "\n",
    "With this we are all set to write the constraints for our optimization model for SVM. \n",
    "### Constraints: \n",
    "$$\n",
    "\\begin{align}\n",
    "&w \\cdot x_i + b \\leq -1, \\quad \\text{if}~y_i = -1\\\\\n",
    "&w \\cdot x_i + b \\geq +1, \\quad \\text{if}~y_i = +1\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, objective function with constraints, gives us the full model. The data for which the label $y_i$ is $-1$ satisfies $w \\cdot x + b \\leq -1,$ and the data for which the lable $y_i$ is $+1$ satisfies $w \\cdot x + b \\geq +1.$ Hence both these conditions can be combined to get\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i (w \\cdot x_i + b) \\geq 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Optimization Model (Primal Form):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} \\quad & \\dfrac{1}{2} \\| w \\|^2 \\\\\n",
    "\\text{subject to} \\quad &y_i(w \\cdot x_i + b) \\geq 1, \\quad i=1,\\dots,m,\n",
    "\\end{align}\n",
    "$$\n",
    "where $m$ is the number of samples $x_i,$ and $w \\in \\mathbb{R}^n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question1:}$ Prove that the primal objective is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ To prove that the primal objective is convex, we need to show that the objective function, which is $\\frac{1}{2} ||w||^2$, is convex, and the constraints, $y_i(w \\cdot x_i + b) \\geq 1$ for $i = 1, \\ldots, m$, are also convex.\n",
    "\n",
    "**Convexity of the Objective Function:**\n",
    "\n",
    "The function $f(w) = \\frac{1}{2} ||w||^2$ is a quadratic function. To prove that it is convex, we need to show that its Hessian matrix is positive semi-definite. The Hessian matrix of $f(w)$ is the matrix of second partial derivatives, denoted as $H$.\n",
    "$$\n",
    "H = \\nabla^2(f(w)) = \\nabla^2(\\frac{1}{2} ||w||^2)\\\\\n",
    "\n",
    "\n",
    "\\implies H = \\nabla(||w||)\\\\\n",
    "\n",
    "\n",
    "\\implies H = I \n",
    "$$\n",
    "The Hessian of this function is simply the identity matrix which is a positive semidefinite matrix. Thus, $H$ is positive definite(all its eigenvalues are positive), which means the objective function is convex. \n",
    "\n",
    "**Convexity of the Constraints:**\n",
    "\n",
    "Each of these constraints is a linear inequality in the variables $w$ and $b$. Linear inequalities are convex, and the left-hand side of these constraints is an affine function (a linear function plus a constant). Affine functions are also convex.\n",
    "\n",
    "\n",
    "Since both the objective function and the constraints are convex, the overall primal problem is convex.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question2}:$ Write the primal problem in standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ The standard form for a convex optimization problem involves expressing the problem with equality constraints. To convert the primal problem into standard form, we introduce slack variables for the inequality constraints and express the constraints as equalities. Here's the primal problem in standard form:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} \\quad & \\frac{1}{2}w^T \\cdot w \\\\\n",
    "\\text{subject to} \\quad &y_i(w \\cdot x_i + b) \\geq 1\\quad \\text{for} \\quad i = 1, \\ldots, m\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "To rewrite these constraints in standard form, we introduce slack variables $s_i$ for each of the inequality constraints to make them equalities:\n",
    "$$\n",
    "y_i(w \\cdot x_i + b) - s_i =  1, \\quad i=1,\\dots,m,\n",
    "$$\n",
    "The optimization problem is now expressed in terms of the variables $w$,$b$ and $s_i$, and all constraints are equalities.\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model (Dual Form)\n",
    "The dual form was derived in lecture 16:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m{\\lambda_i} - \\dfrac{1}{2} \\sum_{i=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad \\sum_{i=1}^m{\\lambda_i y_i} = 0, \\quad i = 1, \\dots, m\n",
    "\\end{align*}, \n",
    "$$\n",
    "where $\\lambda_i$ is the Lagrange multiplier. We claim that strong duality holds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question3:}$ Show the derivation of dual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ \n",
    "**Primal Problem:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{maximize} \\quad L(w, b, \\lambda) = \\frac{1}{2}\\|w\\|^2 - \\sum_{i=1}^m \\lambda_i (y_i (w \\cdot x_i + b) - 1) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The Lagrangian, $L(w, b, \\lambda)$, is the objective function of the primal problem with Lagrange multipliers $\\lambda$ for the inequality constraints.\n",
    "\n",
    "Now, we want to derive the dual form of this problem. First, we need to minimize $L(w, b, \\lambda)$ with respect to $w$ and $b$. To do this, we set the partial derivatives with respect to $w$ and $b$ to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^m \\lambda_i y_i x_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "$$\n",
    "\n",
    "Solving these equations for $w$ and $b$:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^m \\lambda_i y_i x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "$$\n",
    "\n",
    "Now, we can substitute these values back into $L(w, b, \\lambda)$ to obtain the dual Lagrangian, $D(\\lambda)$:\n",
    "\n",
    "$$\n",
    "D(\\lambda) = \\frac{1}{2} \\|\\sum_{i=1}^m \\lambda_i y_i x_i\\|^2 - \\sum_{i=1}^m \\lambda_i (y_i (\\sum_{j=1}^m \\lambda_j y_j x_j \\cdot x_i + b) - 1)\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "D(\\lambda) = \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) - \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) - b\\sum_{i=1}^m \\lambda_i y_i + \\sum_{i=1}^m \\lambda_i\n",
    "$$\n",
    "\n",
    "Since $\\sum_{i=1}^m \\lambda_i y_i = 0$ (from the primal problem), the equation simplifies further:\n",
    "\n",
    "$$\n",
    "D(\\lambda) = \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j)\n",
    "$$\n",
    "\n",
    "**Dual Problem:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad \\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question4:}$ Prove that strong duality holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ To prove that strong duality holds for the given SVM (Support Vector Machine) problem, we need to show that the optimal values of the primal and dual problems are equal.\n",
    "\n",
    "Let's denote the optimal values of the primal and dual problems as follows:\n",
    "\n",
    "Optimal value of the primal problem: $P^*$\n",
    "Optimal value of the dual problem: $D^*$\n",
    "\n",
    "We'll start by proving that $D^* \\leq P^*$, and then we'll prove the reverse inequality. The fact that $D^* = P^*$ will demonstrate strong duality.\n",
    "\n",
    "**1. Proving $D^* \\leq P^*$:**\n",
    "\n",
    "We have the dual problem:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad \\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "\\end{align*}\n",
    "\n",
    "We also have the primal problem:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\frac{1}{2}\\|w\\|^2 - \\sum_{i=1}^m \\lambda_i (y_i (w \\cdot x_i + b) - 1) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{align*}\n",
    "\n",
    "The primal problem is a minimization problem with constraints, and the dual problem is a maximization problem with constraints. The optimal value of the dual problem is always less than or equal to the optimal value of the primal problem, which means $D^* \\leq P^*$.\n",
    "\n",
    "**2. Proving $P^* \\leq D^*$:**\n",
    "\n",
    "Now, let's prove that $P^* \\leq D^*$. We'll use the properties of duality to establish this result. The duality gap, which is the difference between the optimal values of the primal and dual problems, is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "D^* - P^* &= \\left(\\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j)\\right) \\\\\n",
    "&\\quad - \\left(\\frac{1}{2}\\|w\\|^2 - \\sum_{i=1}^m \\lambda_i (y_i (w \\cdot x_i + b) - 1)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "We can rewrite this as:\n",
    "\n",
    "\\begin{align*}\n",
    "D^* - P^* &= \\sum_{i=1}^m \\lambda_i - \\frac{1}{2}\\|w\\|^2 - \\sum_{i=1}^m \\lambda_i (y_i (w \\cdot x_i + b) - 1) \\\\\n",
    "&\\quad + \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j)\n",
    "\\end{align*}\n",
    "\n",
    "Now, we need to use the complementary slackness conditions. For any optimal solution $(w^*, b^*, \\lambda^*)$ of the primal problem and any optimal solution $\\lambda^*$ of the dual problem, these conditions hold:\n",
    "\n",
    "1. For all $i = 1, \\ldots, m$, either $\\lambda_i^* = 0$ or $y_i (w^* \\cdot x_i + b^*) - 1 = 0$.\n",
    "2. For all $i = 1, \\ldots, m$, either $\\lambda_i^* = 0$ or $\\lambda_i^* (y_i (w^* \\cdot x_i + b^*) - 1) = 0$.\n",
    "\n",
    "Using these conditions, we can simplify the duality gap:\n",
    "\n",
    "\\begin{align*}\n",
    "D^* - P^* &= \\sum_{i=1}^m \\lambda_i - \\frac{1}{2}\\|w^*\\|^2 - \\sum_{i=1}^m \\lambda_i (0) \\\\\n",
    "&\\quad + \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j)\n",
    "\\end{align*}\n",
    "\n",
    "Since the complementary slackness conditions imply that $\\lambda_i (y_i (w^* \\cdot x_i + b^*) - 1) = 0$ for all $i$, the second term in the duality gap becomes zero.\n",
    "\n",
    "\\begin{align*}\n",
    "D^* - P^* &= \\sum_{i=1}^m \\lambda_i - \\frac{1}{2}\\|w^*\\|^2 + \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j)\n",
    "\\end{align*}\n",
    "\n",
    "Now, let's consider the term $\\frac{1}{2}\\|w^*\\|^2$. Recall that $w^* = \\sum_{i=1}^m \\lambda_i^* y_i x_i$. Substituting this into the term:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{2}\\|w^*\\|^2 &= \\frac{1}{2}\\left\\|\\sum_{i=1}^m \\lambda_i^* y_i x_i\\right\\|^2\n",
    "\\end{align*}\n",
    "\n",
    "By the properties of the inner product, we can rewrite this term as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i^* \\lambda_j^* y_i y_j (x_i \\cdot x_j)\n",
    "\\end{align*}\n",
    "\n",
    "Now, the duality gap becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "D^* - P^* &= \\sum_{i=1}^m \\lambda_i - \\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i^* \\lambda_j^* y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\quad + \\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i^* \\lambda_j^* y_i y_j (x_i \\cdot x_j)\n",
    "\\end{align*}\n",
    "\n",
    "As you can see, the terms $\\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i^* \\lambda_j^* y_i y_j (x_i \\cdot x_j)$ cancel out, and the duality gap simplifies to:\n",
    "\n",
    "\\begin{align*}\n",
    "D^* - P^* &= \\sum_{i=1}^m \\lambda_i - \\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i^* \\lambda_j^* y_i y_j (x_i \\cdot x_j)\n",
    "\\end{align*}\n",
    "\n",
    "Since we have already established that $D^* \\leq P^*$, and the duality gap is non-negative, this implies that $D^* = P^*$.\n",
    "\n",
    "Therefore, strong duality holds for the SVM problem, which means that the optimal values of the primal and dual problems are equal, and the dual problem provides a tight lower bound on the optimal value of the primal problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question5:}$ Prove that the dual objective is concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ To prove that the dual objective is concave, we need to show that the Hessian matrix of the dual Lagrangian with respect to the vector of dual variables $\\boldsymbol{\\lambda}$ is negative semi-definite. In other words, we need to demonstrate that the second derivative of the dual Lagrangian with respect to $\\lambda_i$ and $\\lambda_j$ is non-positive for all $i$ and $j$.\n",
    "\n",
    "The dual Lagrangian is given as follows:\n",
    "\n",
    "$$\n",
    "D(\\boldsymbol{\\lambda}) = \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j)\n",
    "$$\n",
    "\n",
    "Now, let's calculate the Hessian matrix of $D$ with respect to $\\boldsymbol{\\lambda}$. The Hessian matrix $H$ is defined as follows:\n",
    "\n",
    "$$\n",
    "H_{ij} = \\frac{\\partial^2 D}{\\partial \\lambda_i \\partial \\lambda_j}\n",
    "$$\n",
    "\n",
    "Let's compute the second derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 D}{\\partial \\lambda_i \\partial \\lambda_j} = -y_i y_j (x_i \\cdot x_j)\n",
    "$$\n",
    "\n",
    "So, the Hessian matrix $H$ is:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "-y_1 y_1 (x_1 \\cdot x_1) & -y_1 y_2 (x_1 \\cdot x_2) & \\ldots & -y_1 y_m (x_1 \\cdot x_m) \\\\\n",
    "-y_2 y_1 (x_2 \\cdot x_1) & -y_2 y_2 (x_2 \\cdot x_2) & \\ldots & -y_2 y_m (x_2 \\cdot x_m) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-y_m y_1 (x_m \\cdot x_1) & -y_m y_2 (x_m \\cdot x_2) & \\ldots & -y_m y_m (x_m \\cdot x_m) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we need to show that this Hessian matrix is negative semi-definite, which means that for any vector $\\mathbf{v}$, the following inequality holds:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^T H \\mathbf{v} \\leq 0\n",
    "$$\n",
    "\n",
    "Let $\\mathbf{v} = [\\alpha_1, \\alpha_2, \\ldots, \\alpha_m]$, where $\\alpha_i$ is a scalar. Now, we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^T H \\mathbf{v} = \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j (-y_i y_j (x_i \\cdot x_j))\n",
    "$$\n",
    "\n",
    "Since all the terms in the double sum are negative or zero (because $y_i$, $y_j$, and $(x_i \\cdot x_j)$ are real numbers), the sum is non-positive:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^T H \\mathbf{v} \\leq 0\n",
    "$$\n",
    "\n",
    "This demonstrates that the Hessian matrix $H$ is negative semi-definite, which, in turn, shows that the dual objective is concave. Therefore, we have proved that the dual objective is concave, and this property is essential for convex optimization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question6}:$ Write the dual problem in standard form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer}:$ The dual problem in standard form is as follows:\n",
    "\n",
    "**Dual Problem (Standard Form):**\n",
    "\\begin{align*}\n",
    "&\\text{maximize} \\quad \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j y_i y_j (x_i \\cdot x_j) \\\\\n",
    "&\\text{subject to} \\quad \\lambda_i \\geq 0, \\quad \\sum_{i=1}^m \\lambda_i y_i = 0\n",
    "\\end{align*}\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin SVM\n",
    "In a variant of soft margin SVM, we assume that some data samples may be outliers or noise, and this prevents the data from being linearly separable. For example, see the figure below\n",
    "\n",
    "<img src=\"svmt4.png\" width=\"400\">\n",
    "\n",
    "In the figure, we see that \n",
    "\n",
    "- We believe that two red and one blue sample is noisy or outliers.\n",
    "- We now want to take into account that real life data is noisy, we decide to allow for some of the noisy data in the margin.\n",
    "- Let $\\xi_i$ denotes how far a data sample is from the middle plane (margin is the area between dotted line).\n",
    "- For example, one of the noisy red data point in 0.6 away from middle red plane. \n",
    "- We introduce this slack variable $\\xi_i \\geq 0$ for each data sample $x_i.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Primal Soft-Margin\n",
    "We can then write the primal soft-margin optimization model as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{minimize} \\quad \\dfrac{1}{2} \\| w \\|^2 + C \\sum_{i=1}^m \\xi_i \\\\\n",
    "&\\text{subject to} \\quad y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\dots, m.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Dual Soft-Margin\n",
    "We can also write the dual form of soft-margin SVM as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad &\\sum_{i=1}^m \\lambda_i - \\dfrac{1}{2} \\sum_{i,j=1}^m \\lambda_i \\lambda_j \\: y_i y_j \\: x_i \\cdot x_j \\\\\n",
    "\\text{subject to} \\quad &0 \\leq \\lambda_i \\leq C, \\quad  i=1, \\dots, m, \\\\ \n",
    "&\\sum_{i=1}^m \\lambda_i y_i = 0.\t \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question7:}$ Show the derivation of dual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ The dual problem is obtained by solving the Lagrangian of the primal problem, and then finding its dual by maximizing it. Here's the derivation step by step:\n",
    "\n",
    "Step 1: Lagrangian Formulation of Primal Problem\n",
    "The primal soft-margin SVM optimization problem can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimize} \\quad &\\frac{1}{2} | w |^2 + C \\sum_{i=1}^m \\xi_i \\\n",
    "\\text{subject to} \\quad &y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\dots, m.\n",
    "\\end{align*}\n",
    "\n",
    "We introduce Lagrange multipliers, denoted as $\\lambda_i$ for the inequality constraints and $\\alpha_i$ for the non-negativity constraints ($\\xi_i \\geq 0$), to form the Lagrangian:\n",
    "\n",
    "\\begin{align*}\n",
    "L(w, b, \\xi, \\lambda, \\alpha) = \\frac{1}{2} | w |^2 + C \\sum_{i=1}^m \\xi_i - \\sum_{i=1}^m \\lambda_i \\left(y_i (w \\cdot x_i + b) - 1 + \\xi_i\\right) - \\sum_{i=1}^m \\alpha_i \\xi_i.\n",
    "\\end{align*}\n",
    "\n",
    "Step 2: Setting the Gradient to Zero\n",
    "To find the dual problem, we need to find the saddle point of the Lagrangian by setting its gradient with respect to the primal variables (w, b, and $\\xi_i$) equal to zero. This gives us the following equations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_w L &= w - \\sum_{i=1}^m \\lambda_i y_i x_i = 0 \\\n",
    "\\frac{\\partial L}{\\partial b} &= -\\sum_{i=1}^m \\lambda_i y_i = 0 \\\n",
    "\\frac{\\partial L}{\\partial \\xi_i} &= C - \\lambda_i - \\alpha_i = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Step 3: Solving for w and b\n",
    "From the first equation, we can solve for w:\n",
    "\n",
    "\\begin{align*}\n",
    "w = \\sum_{i=1}^m \\lambda_i y_i x_i.\n",
    "\\end{align*}\n",
    "\n",
    "From the second equation, we can see that the sum of Lagrange multipliers times the labels is zero, which implies that the support vectors have non-zero Lagrange multipliers.\n",
    "\n",
    "Step 4: Substituting w and b back into the Lagrangian\n",
    "Substitute the expressions for w and b back into the Lagrangian:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\lambda, \\alpha) = \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i,j=1}^m \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j - b \\sum_{i=1}^m \\lambda_i y_i - \\sum_{i=1}^m \\lambda_i \\xi_i - \\sum_{i=1}^m \\alpha_i \\xi_i.\n",
    "\\end{align*}\n",
    "\n",
    "Step 5: The Dual Problem\n",
    "\n",
    "Now, the dual problem is to maximize this Lagrangian with respect to $\\lambda_i$ subject to the constraints:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad &\\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i,j=1}^m \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j \\\n",
    "\\text{subject to} \\quad &0 \\leq \\lambda_i \\leq C, \\quad i=1, \\dots, m \\\n",
    "&\\sum_{i=1}^m \\lambda_i y_i = 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question8:}$ List advantages of dual over primal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ The dual form of the support vector machine (SVM) problem offers several advantages over the primal form in certain situations. Here are some of the advantages of the dual form over the primal form of SVM:\n",
    "\n",
    "1. The dual form has a **simpler set of constraints**. In the primal form, we have both equality and inequality constraints, which can make it more complex to solve and potentially slower. The dual form, on the other hand, has only inequality constraints, making it easier to work with.\n",
    "\n",
    "2. In high-dimensional feature spaces, it is often **computationally more efficient** to work in the dual space. This is because the dual form involves inner products between data points, which can be efficiently computed using kernel functions without explicitly representing the transformed feature vectors.\n",
    "\n",
    "3.  The dual form often leads to a convex quadratic programming problem, which has a unique global solution and can be efficiently solved using various optimization techniques. In contrast, the primal form may lead to a non-convex problem.\n",
    "\n",
    "4. In the dual form, support vectors are explicitly associated with non-zero Lagrange multipliers (the $\\lambda$ values). This makes it easier to identify support vectors, which are critical for defining the optimal hyperplane.\n",
    "\n",
    "5. The regularization parameter C, which controls the trade-off between maximizing the margin and minimizing classification errors, is often more straightforward to incorporate into the dual form. It directly affects the upper bound of the Lagrange multipliers, making it easy to control model complexity.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels in SVM\n",
    "## Non-Linear Classifiers\n",
    "- For nonlinear data, we may map the data to a higher dimensional feature space where it is separable. See the figure below:\n",
    "\n",
    "<img src=\"svmt5.png\" width=\"700\">\n",
    "\n",
    "Such non-linear transformation can be implemented more effectively using the dual formulation. \n",
    "- If we solve the dual form of linear SVM, then the predictions is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= \\text{sign}(w \\cdot x + b) \\\\\n",
    "w &= \\sum_{i=1}^m \\alpha_i y_i x_i \n",
    "\\end{align*}\n",
    "$$\n",
    "If we assume that we did some transform $\\Phi,$ then the classifier is given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= \\text{sign} (w \\cdot \\Phi(x) + b) \\\\\n",
    "w &= \\sum_{i=1}^m \\alpha_i y_i \\Phi(x_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "If we substitute $w$ in $f(x),$ we observe that\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) = \\text{sign} \\left ( \\sum_{i=1}^m \\alpha_i y_i \\, \\Phi(x_i) \\cdot \\Phi(x) + b \\right) = \\text{sign} \\left( \\sum_{i=1}^m \\alpha_i y_i \\, K(x_i, x) + b \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "Note that doing dot products such as $\\Phi(x_i) \\cdot \\Phi(x),$ if $\\Phi(x)$ is a long vector! An important observation is to define this dot product or $K(x,z)$ such that dot products happen in input space rather than the feature space. We can see this with following example:\n",
    "$$\n",
    "\\begin{align*}\n",
    "K(x \\cdot z) &= (x \\cdot z)^2 = \\left( \\begin{bmatrix}\n",
    "x_{(1)} \\\\ x_{(2)} \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "z_{(1)} \\\\ z_{(2)}\n",
    "\\end{bmatrix} \\right)^2 = (x_{(1)} z_{(1)} + x_{(2)} z_{(2)})^2 \\\\\n",
    "&= x_{(1)}^2 z_{(1)}^2 + 2x_{(1)} z_{(1)} x_{(2)} z_{(2)} + x_{(2)}^2 z_{(2)}^2 = \\begin{bmatrix}\n",
    "x_{(1)}^2 \\\\ \\sqrt{2} x_{(1)} x_{(2)} \\\\ x_{(2)}^2 \n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "z_{(1)}^2 \\\\ \\sqrt{2} z_{(1)} z_{(2)} \\\\ z_{(2)}^2 \n",
    "\\end{bmatrix}  \\\\\n",
    "&= \\Phi(x) \\cdot \\Phi(z)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question9:}$ Let the kernel be defined by $K(x,z) = (x \\cdot z)^3.$ Define $\\Phi(x).$ Assuming that one multiplications is 1 FLOP, and one addition is 1 FLOP, then how many flops you need to compute $K(x \\cdot z)$ in input space versus feature space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ The kernel function is defined as:\n",
    "\n",
    "$$K(x, z) = (x \\cdot z)^3$$\n",
    "\n",
    "To find the corresponding feature map $\\Phi(x)$, we can expand $K(x, z)$ and represent it as an inner product of two transformed vectors:\n",
    "\n",
    "$$K(x, z) = (x_1z_1 + x_2z_2)^3$$\n",
    "\n",
    "Now, let's expand this using the binomial theorem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "K(x, z) &= (x_1z_1 + x_2z_2)^3 \\\\\n",
    "&= (x_1z_1)^3 + 3(x_1z_1)^2(x_2z_2) + 3(x_1z_1)(x_2z_2)^2 + (x_2z_2)^3\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can see that this expansion consists of terms that are products of $x_i$ and $z_i$ raised to various powers. Now, we can define the feature map $\\Phi(x)$ corresponding to this kernel:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\begin{bmatrix} (x_1z_1)^3 \\\\ 3(x_1z_1)^2(x_2z_2) \\\\ 3(x_1z_1)(x_2z_2)^2 \\\\ (x_2z_2)^3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the feature map $\\Phi(x)$ transforms the input $x$ into a higher-dimensional space.\n",
    "\n",
    "Now, let's compare the number of FLOPs needed to compute $K(x \\cdot z)$ in input space and feature space.\n",
    "\n",
    "In the input space, to compute $K(x \\cdot z)$, you need to perform three multiplications ($x_1z_1$, $x_1z_2$, and $x_2z_2$), one addition for the inner product, and two additional multiplications for the cube:\n",
    "\n",
    "$$\n",
    "\\text{FLOPs in input space} = 3\\text{ multiplications} + 1\\text{ addition} + 2\\text{ multiplications} = 6\\text{ FLOPs}\n",
    "$$\n",
    "\n",
    "In the feature space, to compute $\\Phi(x) \\cdot \\Phi(z)$, you need to perform four multiplications and three additions:\n",
    "\n",
    "$$\n",
    "\\text{FLOPs in feature space} = 4\\text{ multiplications} + 3\\text{ additions} = 7\\text{ FLOPs}\n",
    "$$\n",
    "\n",
    "So, in this specific case, it requires 6 FLOPs to compute $K(x \\cdot z)$ in the input space and 7 FLOPs to compute $\\Phi(x) \\cdot \\Phi(z)$ in the feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Model: Dual Soft Margin Kernel SVM\n",
    "We can now write the dual form of soft-margin Kernel SVM as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad &\\sum_{i=1}^m \\lambda_i - \\dfrac{1}{2} \\sum_{i, \\, j=1}^m \\lambda_i \\lambda_j \\: y_i y_j \\: \\Phi(x_i) \\cdot \\Phi(x_j) \\\\\n",
    "\\text{subject to} \\quad &0 \\leq \\lambda_i \\leq C, \\quad  i=1, \\dots, m, \\\\ \n",
    "&\\sum_{i=1}^m \\lambda_i y_i = 0.\t \n",
    "\\end{align*}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver for Optimization Problem: Quadratic Programming\n",
    "We aspire to solve the above optimization problem using existing quadraric programming library. But we have a problem: the standard libraries use the standard form of the quadratic optimization problem that looks like the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimize} \\quad &\\dfrac{1}{2} x^T P x + q^T x, \\\\ \n",
    "\\text{subject to} \\quad &Gx \\leq h, \\\\\n",
    "&Ax = b\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Soft-Margin Kernel SVM in Standard QP: Assemble Matrices Vectors\n",
    "To put the dual Kernel SVM in standard form, we need to set\n",
    "- matrix $P$\n",
    "- vector $x$\n",
    "- vector $q$\n",
    "- vector $h$\n",
    "- vector $b$\n",
    "- matrix $G$\n",
    "- matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix $P$\n",
    "Let $$K(x_i, x_j) = \\Phi(x_i) \\cdot \\Phi(x_j),$$ and set $(i,j)$ entry of matrix $P$ as $$P_{ij} = y_iy_j K(x_i,x_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $x$\n",
    "Set\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "\\lambda_1 \\\\\n",
    "\\lambda_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_m\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $q$\n",
    "Set $q \\in \\mathbb{R}^m$\n",
    "$$ q = \n",
    "\\begin{bmatrix}\n",
    "-1 \\\\ -1 \\\\ \\vdots \\\\ -1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix $A$\n",
    "Set the matrix (in fact vector) $A$ as \n",
    "$$\n",
    "A = [y_1, y_2, \\dots, y_m]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $b$\n",
    "In fact vector $b$ is a scalar here: $$b = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix $G$\n",
    "$$\n",
    "\\begin{align*}\n",
    "G = \\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\dots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 1 \\\\ \\hline\n",
    "-1 & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\dots & \\vdots \\\\\n",
    "0 & 0 & \\dots& -1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector $h$\n",
    "Set $h$ as \n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "C \\\\\n",
    "C \\\\\n",
    "\\vdots \\\\\n",
    "C \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Kernel SVM\n",
    "We are all set to try out coding the classifier using Kernel SVM. We will first import some libraries. Some of these libraries may not be available in your system. You may install them as follows:\n",
    "- conda install numpy\n",
    "- conda install -c conda-forge cvxopt\n",
    "- sudo apt-get install python-scipy python-matplotlib\n",
    "\n",
    "Try google search, if these does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import cvxopt as cvxopt\n",
    "from cvxopt import solvers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a class: svm \n",
    "This class will have the following functions:\n",
    "- __init__: where we will define initial default parameters\n",
    "- *construct_kernel*: here we will define some kernels such as polynomial and RBF (radial basis or Gaussian kernel)\n",
    "- *train_kernel_svm*: Here we will train, i.e, we will call a quadratic programming solver from cvxopt\n",
    "- *classify*: Here we will test our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question10:}$ Fill the TODO below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm:\n",
    "\n",
    "    def __init__(self, kernel='linear', C=None, sigma=1., degree=1., threshold=1e-5):\n",
    "        self.kernel = kernel\n",
    "        if self.kernel == 'linear':\n",
    "            self.degree = 1.\n",
    "            self.kernel = 'poly'\n",
    "            \n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        self.threshold = threshold\n",
    "        self.degree = degree\n",
    "        \n",
    "\n",
    "    def construct_kernel(self, X):\n",
    "        self.K = np.dot(X, X.T)\n",
    "\n",
    "        if self.kernel == 'poly':\n",
    "            self.K = (1. + 1./self.sigma*self.K)**self.degree\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            self.xsquared = (np.diag(self.K)*np.ones((1, self.N))).T\n",
    "            b = np.ones((self.N, 1))\n",
    "            self.K -= 0.5*(np.dot(self.xsquared, b.T) +\n",
    "                           np.dot(b, self.xsquared.T))\n",
    "            self.K = np.exp(self.K/(2.*self.sigma**2))\n",
    "\n",
    "    def train_kernel_svm(self, X, targets):\n",
    "        self.N = np.shape(X)[0]\n",
    "        self.construct_kernel(X)\n",
    "\n",
    "        # Assemble the matrices for the constraints   \n",
    "        P = np.dot(np.dot(targets, targets.T) * self.K, self.K.T)\n",
    "        q = -np.ones((self.N, 1))\n",
    "        G = -np.eye(self.N)\n",
    "        h = np.zeros((self.N, 1))\n",
    "        A = targets.reshape(1, -1)\n",
    "        b = 0.0\n",
    "\n",
    "        # Call the the quadratic solver of cvxopt library.\n",
    "        sol = cvxopt.solvers.qp(cvxopt.matrix(P), cvxopt.matrix(q), cvxopt.matrix(\n",
    "            G), cvxopt.matrix(h), cvxopt.matrix(A), cvxopt.matrix(b))\n",
    "\n",
    "        # Get the Lagrange multipliers out of the solution dictionary\n",
    "        lambdas = np.array(sol['x'])\n",
    "\n",
    "        # Find the (indices of the) support vectors, which are the vectors with non-zero Lagrange multipliers\n",
    "        self.sv = np.where(lambdas > self.threshold)[0]\n",
    "        self.nsupport = len(self.sv)\n",
    "        print (\"Number of support vectors = \", self.nsupport)\n",
    "\n",
    "        # Keep the data corresponding to the support vectors\n",
    "        self.X = X[self.sv, :]\n",
    "        self.lambdas = lambdas[self.sv]\n",
    "        self.targets = targets[self.sv]\n",
    "\n",
    "        self.b = np.sum(self.targets)\n",
    "        for n in range(self.nsupport):\n",
    "            self.b -= np.sum(self.lambdas*self.targets *\n",
    "                             np.reshape(self.K[self.sv[n], self.sv], (self.nsupport, 1)))\n",
    "        self.b /= len(self.lambdas)\n",
    "\n",
    "        if self.kernel == 'poly':\n",
    "            def classify(Y, soft=False):\n",
    "                K = (1. + 1./self.sigma*np.dot(Y, self.X.T))**self.degree\n",
    "\n",
    "                self.y = np.zeros((np.shape(Y)[0], 1))\n",
    "                for j in range(np.shape(Y)[0]):\n",
    "                    for i in range(self.nsupport):\n",
    "                        self.y[j] += self.lambdas[i]*self.targets[i]*K[j, i]\n",
    "                    self.y[j] += self.b\n",
    "\n",
    "                if soft:\n",
    "                    return self.y\n",
    "                else:\n",
    "                    return np.sign(self.y)\n",
    "\n",
    "        elif self.kernel == 'rbf':\n",
    "            def classify(Y, soft=False):\n",
    "                K = np.dot(Y, self.X.T)\n",
    "                c = (1./self.sigma * np.sum(Y**2, axis=1)\n",
    "                     * np.ones((1, np.shape(Y)[0]))).T\n",
    "                c = np.dot(c, np.ones((1, np.shape(K)[1])))\n",
    "                aa = np.dot(self.xsquared[self.sv],\n",
    "                            np.ones((1, np.shape(K)[0]))).T\n",
    "                K = K - 0.5*c - 0.5*aa\n",
    "                K = np.exp(K/(2.*self.sigma**2))\n",
    "\n",
    "                self.y = np.zeros((np.shape(Y)[0], 1))\n",
    "                for j in range(np.shape(Y)[0]):\n",
    "                    for i in range(self.nsupport):\n",
    "                        self.y[j] += self.lambdas[i]*self.targets[i]*K[j, i]\n",
    "                    self.y[j] += self.b\n",
    "\n",
    "                if soft:\n",
    "                    return self.y\n",
    "                else:\n",
    "                    return np.sign(self.y)\n",
    "        else:\n",
    "            print (\"Error: Invalid kernel\")\n",
    "            return\n",
    "\n",
    "        self.classify = classify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P`: The matrix for the quadratic term in the quadratic programming problem.\n",
    "\n",
    "`q`: The matrix for the linear term in the quadratic programming problem.\n",
    "\n",
    "`G`: The matrix for the inequality constraint (inequality: Gx <= h).\n",
    "\n",
    "`h`: The vector for the right-hand side of the inequality constraint.\n",
    "\n",
    "`A`: The matrix for the equality constraint (equality: Ax = b).\n",
    "\n",
    "`b`: The vector for the right-hand side of the equality constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question11:}$ How $b$ was computed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ The value of $b$ is computed using the following formula:\n",
    "$$\n",
    "b = \\frac{1}{N_{\\text{support}}} \\sum_{i=1}^{N_{\\text{support}}} \\left( \\text{targets}[i] - \\sum_{j=1}^{N_{\\text{support}}} \\lambda_j \\cdot \\text{targets}[j] \\cdot K[\\text{sv}[i], \\text{sv}[j]] \\right)\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text{Where:}\n",
    "$\n",
    "\n",
    "$\n",
    "b \\text{ is the bias term.}\n",
    "$\n",
    "\n",
    "$\n",
    "N_{\\text{support}} \\text{ is the number of support vectors.}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{targets}[i] \\text{ is the target value for the } i^{th} \\text{ support vector.}\n",
    "$\n",
    "\n",
    "$\n",
    "\\lambda_j \\text{ is the Lagrange multiplier for the } j^{th} \\text{ support vector.}\n",
    "$\n",
    "\n",
    "$\n",
    "K[\\text{sv}[i], \\text{sv}[j]] \\text{ is the kernel matrix entry between the } i^{th} \\text{ and } j^{th} \\text{ support vectors.}\n",
    "$\n",
    "\n",
    "The code already provided computes $b$ correctly in the following lines:\n",
    "\n",
    "``` code\n",
    "self.b = np.sum(self.targets)\n",
    "for n in range(self.nsupport):\n",
    "    self.b -= np.sum(self.lambdas*self.targets *\n",
    "                     np.reshape(self.K[self.sv[n], self.sv], (self.nsupport, 1)))\n",
    "self.b /= len(self.lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Classifier\n",
    "In the following, we will now test our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import sklearn.svm as svm\n",
    "\n",
    "\n",
    "iris = np.loadtxt('iris_proc.data', delimiter=',')\n",
    "imax = np.concatenate((iris.max(axis=0)*np.ones((1, 5)),\n",
    "                       iris.min(axis=0)*np.ones((1, 5))), axis=0).max(axis=0)\n",
    "\n",
    "target = -np.ones((np.shape(iris)[0], 3), dtype=float)\n",
    "indices = np.where(iris[:, 4] == 0)\n",
    "target[indices, 0] = 1.\n",
    "indices = np.where(iris[:, 4] == 1)\n",
    "target[indices, 1] = 1.\n",
    "indices = np.where(iris[:, 4] == 2)\n",
    "target[indices, 2] = 1.\n",
    "\n",
    "train = iris[::2, 0:4]\n",
    "traint = target[::2]\n",
    "test = iris[1::2, 0:4]\n",
    "testt = target[1::2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the machines\n",
    "output = np.zeros((np.shape(test)[0], 3))\n",
    "\n",
    "# Train for the first set of train data with a linear kernel\n",
    "svm0 = svm.SVC(kernel='linear')\n",
    "svm0.fit(train, traint[:, 0])\n",
    "output[:, 0] = svm0.decision_function(test)\n",
    "\n",
    "# Train for the second set of train data with a polynomial kernel\n",
    "svm1 = svm.SVC(kernel='poly', C=0.1, degree=1)\n",
    "svm1.fit(train, traint[:, 1])\n",
    "output[:, 1] = svm1.decision_function(test)\n",
    "\n",
    "# Train for the third set of train data with an RBF kernel\n",
    "svm2 = svm.SVC(kernel='rbf')\n",
    "svm2.fit(train, traint[:, 2])\n",
    "output[:, 2] = svm2.decision_function(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 1 2 2 2 1 1 1 2\n",
      " 2 2 1 1 2 2 2 1 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2.]\n",
      "Misclassified locations:\n",
      "[25 27 30 31 32 36 37 38 41 42 43 45]\n",
      "0.84 test accuracy\n"
     ]
    }
   ],
   "source": [
    "# Make a decision about which class\n",
    "# Pick the one with the largest margin\n",
    "bestclass = np.argmax(output, axis=1)\n",
    "print(bestclass)\n",
    "print(iris[1::2, 4])\n",
    "print(\"Misclassified locations:\")\n",
    "err = np.where(bestclass != iris[1::2, 4])[0]\n",
    "print(err)\n",
    "print(float(np.shape(testt)[0] - len(err)) /\n",
    "       (np.shape(testt)[0]), \"test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question12}:$ The IRIS dataset has three classes. Explain by observing the code above how the two class SVM was modified for multiclass classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ The code demonstrates the use of Support Vector Machines (SVMs) for multiclass classification on the Iris dataset. In multiclass classification, where there are more than two classes, one common approach is to use a \"one-vs-all\" (also known as \"one-vs-rest\") strategy. In this strategy, we train multiple binary classifiers, each of which distinguishes one class from the rest of the classes. Here's how the code achieves this for the Iris dataset:\n",
    "\n",
    "**Data Preparation:**\n",
    "- The Iris dataset has three classes, labeled as 0, 1, and 2.\n",
    "\n",
    "- The variable `target` is created as a matrix of shape (number of samples, number of classes), which is initialized with -1. The size of the last dimension is 3 to accommodate the three classes.\n",
    "\n",
    "- For each class (0, 1, 2), the code sets the corresponding column in target to 1, indicating which class a particular sample belongs to. This creates a one-hot encoded representation for the classes.\n",
    "\n",
    "**Training Multiple Binary Classifiers:**\n",
    "\n",
    "- The code trains three binary SVM classifiers, one for each class (0, 1, and 2). These classifiers are named `svm0`, `svm1`, and `svm2`.\n",
    "\n",
    "- For each binary classifier, the training data (`train`) and the corresponding one-hot encoded target for that class (e.g., `traint[:, 0]` for class 0) are used to train the SVM.\n",
    "\n",
    "- The code uses different kernel functions (e.g., 'rbf') for each SVM, which can have different decision boundaries.\n",
    "\n",
    "**Classification:**\n",
    "\n",
    "- After training the three binary classifiers, the code applies them to the test data (`test`) to get soft classification scores for each class. These scores are stored in the `output` matrix.\n",
    "\n",
    "- `output[:, 0]` contains the soft scores for class 0, `output[:, 1]` for class 1, and `output[:, 2]` for class 2.\n",
    "\n",
    "**Decision Making:**\n",
    "\n",
    "- To make a final decision about which class a test sample belongs to, the code selects the class with the largest margin, which is achieved by taking the class with the highest score in the output matrix. This is done using `np.argmax(output, axis=1)`.\n",
    "\n",
    "**Printing Results:**\n",
    "\n",
    "- The code prints the predicted class (`bestclass`), the actual class from the test data (`iris[1::2, 4]`), and the misclassified locations.\n",
    "\n",
    "- It also calculates and prints the test accuracy by comparing the predicted class with the actual class.\n",
    "\n",
    "Thus the code applies **a one-vs-all** strategy to transform the multiclass problem into multiple binary classification tasks. Each binary classifier is trained to distinguish one class from the others, and the final decision is made by selecting the class with the highest classification score. This approach allows the use of binary SVM classifiers to handle multiclass problems like the Iris dataset, which has three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question13}:$ Write mathematical expressions for the kernels defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ \n",
    "1. **Linear Kernel (svm0 and svm1):**\n",
    "\n",
    "The linear kernel is defined as the dot product between two feature vectors. The expression for the linear kernel (\\(K_{\\text{linear}}\\)) is:\n",
    "    $$K_{\\text{linear}}(x, y) = x^T \\cdot y$$\n",
    "\n",
    "2. **Polynomial Kernel (svm2):**\n",
    "\n",
    "The polynomial kernel is defined as the power of the linear kernel with a specified degree (\\(d\\)). The expression for the polynomial kernel $K_\\text{poly}$ with degree \\(d\\) is:\n",
    "   \n",
    "$$K_{\\text{poly}}(x, y) = (x^T \\cdot y + 1)^d$$\n",
    "   \n",
    "In the code, \\(d\\) is set to 3.\n",
    "\n",
    "3. **Radial Basis Function (RBF) Kernel (svm0, svm1, svm2):**\n",
    "\n",
    "The RBF kernel, also known as the Gaussian kernel, is defined as a Gaussian radial basis function of the Euclidean distance between the feature vectors, with a parameter $\\gamma$. The expression for the RBF kernel $K_\\text{poly}$ is:\n",
    "   \n",
    "$$K_{\\text{RBF}}(x, y) = \\exp\\left(-\\gamma \\cdot ||x - y||^2\\right)$$\n",
    "   \n",
    "Here, $\\gamma$ is a hyperparameter that controls the shape of the kernel function. In the code, $\\gamma$ is not explicitly defined, so its default value is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question14}:$ Play with different Kernels. Which kernels (polynomial, RBF, or linear) give the best test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{Answer:}$ \n",
    "To determine which kernel (polynomial, RBF, or linear) gives the best test accuracy, we can evaluate the accuracy of each kernel on the iris dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for linear kernel: 1.00\n",
      "Accuracy for poly kernel: 1.00\n",
      "Accuracy for rbf kernel: 1.00\n",
      "The best kernel is linear with a test accuracy of 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.loadtxt('iris_proc.data', delimiter=',')\n",
    "X = data[:, :4]  # Features (assumes the first 4 columns are features)\n",
    "y = data[:, 4]   # Labels (assumes the 5th column contains class labels)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    clf = svm.SVC(kernel=kernel)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    accuracies.append((kernel, accuracy))\n",
    "    print(f\"Accuracy for {kernel} kernel: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "best_kernel, best_accuracy = max(accuracies, key=lambda x: x[1])\n",
    "\n",
    "print(f\"The best kernel is {best_kernel} with a test accuracy of {best_accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
